#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXivæ™ºèƒ½è®ºæ–‡è¿½è¸ªå™¨
åŸºäºèƒ¶æ°´ç¼–ç¨‹åŸåˆ™ï¼šèƒ½æŠ„ä¸å†™ï¼Œèƒ½è¿ä¸é€ ï¼Œèƒ½å¤ç”¨ä¸åŸåˆ›
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXivæ™ºèƒ½è®ºæ–‡è¿½è¸ªå™¨
åŸºäºèƒ¶æ°´ç¼–ç¨‹åŸåˆ™ï¼šèƒ½æŠ„ä¸å†™ï¼Œèƒ½è¿ä¸é€ ï¼Œèƒ½å¤ç”¨ä¸åŸåˆ›
"""

import argparse
import json
import os
import sys
import warnings
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Set
from unittest.mock import patch
import urllib3
import requests

# å¿…é¡»åœ¨å¯¼å…¥arxivä¹‹å‰è®¾ç½®SSLç›¸å…³ç¯å¢ƒå˜é‡
def setup_ssl_context(verify_ssl: bool):
    """è®¾ç½®SSLä¸Šä¸‹æ–‡ - å¿…é¡»åœ¨import arxivä¹‹å‰è°ƒç”¨"""
    if not verify_ssl:
        os.environ['PYTHONHTTPSVERIFY'] = '0'
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

import arxiv
from tqdm import tqdm
import pandas as pd


class ArxivTracker:
    """arXivè®ºæ–‡è¿½è¸ªå™¨ - èƒ¶æ°´ä»£ç æ ¸å¿ƒç±»"""

    def __init__(self, config: Dict):
        self.config = config
        self.client = arxiv.Client(
            page_size=100,
            delay_seconds=3.0,
            num_retries=5
        )
        self.history_file = Path(config.get('history_file', 'papers_history.json'))
        self.seen_ids = self._load_history()
        self.results = []

    def _load_history(self) -> Set[str]:
        """åŠ è½½å†å²è®°å½•ç”¨äºå»é‡"""
        if self.history_file.exists():
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
            except (json.JSONDecodeError, IOError):
                return set()
        return set()

    def _save_history(self):
        """ä¿å­˜å†å²è®°å½•"""
        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(list(self.seen_ids), f, indent=2, ensure_ascii=False)

    def _build_query(self) -> str:
        """æ„å»ºarXivæŸ¥è¯¢å­—ç¬¦ä¸²"""
        query_parts = []

        # æ·»åŠ åˆ†ç±»è¿‡æ»¤
        if self.config.get('categories'):
            cat_query = ' OR '.join([f"cat:{cat}" for cat in self.config['categories']])
            query_parts.append(f"({cat_query})")

        # æ·»åŠ å…³é”®è¯æœç´¢
        if self.config.get('keywords'):
            keyword_fields = self.config.get('keyword_fields', ['all'])
            keyword_query = ' OR '.join([
                f"{field}:{kw}" for field in keyword_fields for kw in self.config['keywords']
            ])
            query_parts.append(f"({keyword_query})")

        return ' AND '.join(query_parts) if query_parts else 'cat:cs.*'

    def search_papers(self) -> List[arxiv.Result]:
        """æœç´¢è®ºæ–‡ - ä½¿ç”¨arxivå®˜æ–¹SDK"""
        query_str = self._build_query()

        # ä½¿ç”¨å®˜æ–¹SDKæ„å»ºæœç´¢
        search = arxiv.Search(
            query=query_str,
            max_results=self.config.get('max_results', 10),
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending
        )

        # è·å–ç»“æœå¹¶æ˜¾ç¤ºè¿›åº¦
        results = []
        total = self.config.get('max_results', 10)

        print(f"\nğŸ” æ­£åœ¨æœç´¢arXivè®ºæ–‡...")
        print(f"ğŸ“‹ æŸ¥è¯¢è¯­å¥: {query_str}")

        for result in tqdm(self.client.results(search), total=total, desc="è·å–è®ºæ–‡"):
            # å»é‡è¿‡æ»¤
            if result.entry_id.split('/')[-1] not in self.seen_ids:
                results.append(result)
                self.seen_ids.add(result.entry_id.split('/')[-1])

        # ä¿å­˜å†å²
        self._save_history()

        print(f"âœ… æ‰¾åˆ° {len(results)} ç¯‡æ–°è®ºæ–‡\n")

        return results

    def parse_paper(self, result: arxiv.Result) -> Dict:
        """è§£æè®ºæ–‡ä¿¡æ¯"""
        authors = ', '.join([author.name for author in result.authors[:3]])
        if len(result.authors) > 3:
            authors += f" et al. ({len(result.authors)} authors)"

        return {
            'arxiv_id': result.entry_id.split('/')[-1],
            'title': result.title,
            'authors': authors,
            'summary': result.summary.replace('\n', ' ')[:300] + '...',
            'published': result.published.strftime('%Y-%m-%d'),
            'categories': ', '.join(result.categories),
            'url': result.entry_id,
            'pdf_url': result.pdf_url
        }

    def output_results(self, papers: List[Dict]):
        """å¤šæ ¼å¼è¾“å‡ºç»“æœ"""
        if not papers:
            print("ğŸ“­ æš‚æ— æ–°è®ºæ–‡")
            return

        df = pd.DataFrame(papers)
        date_str = datetime.now().strftime('%Y-%m-%d')
        output_dir = Path(self.config.get('output_dir', 'outputs'))
        output_dir.mkdir(exist_ok=True)

        output_formats = self.config.get('output_formats', ['console'])

        for fmt in output_formats:
            if fmt == 'console':
                self._print_to_console(papers)
            elif fmt == 'txt':
                self._save_txt(df, output_dir, date_str)
            elif fmt == 'md':
                self._save_markdown(papers, output_dir, date_str)
            elif fmt == 'csv':
                df.to_csv(output_dir / f'{date_str}_results.csv', index=False, encoding='utf-8')
                print(f"ğŸ’¾ CSVå·²ä¿å­˜: {output_dir / f'{date_str}_results.csv'}")
            elif fmt == 'json':
                df.to_json(output_dir / f'{date_str}_results.json', orient='records',
                          force_ascii=False, indent=2)
                print(f"ğŸ’¾ JSONå·²ä¿å­˜: {output_dir / f'{date_str}_results.json'}")

    def _print_to_console(self, papers: List[Dict]):
        """ç»ˆç«¯æ ¼å¼åŒ–è¾“å‡º"""
        print("=" * 80)
        print(f"ğŸ“š arXivè®ºæ–‡è¿½è¸ªç»“æœ - {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        print("=" * 80)

        for i, paper in enumerate(papers, 1):
            print(f"\nã€{i}ã€‘{paper['title']}")
            print(f"    ID: {paper['arxiv_id']}")
            print(f"    ä½œè€…: {paper['authors']}")
            print(f"    å‘å¸ƒ: {paper['published']} | åˆ†ç±»: {paper['categories']}")
            print(f"    æ‘˜è¦: {paper['summary']}")
            print(f"    é“¾æ¥: {paper['url']}")
            print(f"    PDF: {paper['pdf_url']}")

        print("\n" + "=" * 80)

    def _save_txt(self, df: pd.DataFrame, output_dir: Path, date_str: str):
        """ä¿å­˜ä¸ºTXTæ ¼å¼"""
        filepath = output_dir / f'{date_str}_results.txt'
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"arXivè®ºæ–‡è¿½è¸ªç»“æœ - {date_str}\n")
            f.write("=" * 80 + "\n\n")
            for _, row in df.iterrows():
                f.write(f"ã€{row['arxiv_id']}ã€‘{row['title']}\n")
                f.write(f"ä½œè€…: {row['authors']}\n")
                f.write(f"å‘å¸ƒ: {row['published']} | åˆ†ç±»: {row['categories']}\n")
                f.write(f"æ‘˜è¦: {row['summary']}\n")
                f.write(f"é“¾æ¥: {row['url']}\n")
                f.write(f"PDF: {row['pdf_url']}\n\n")
        print(f"ğŸ’¾ TXTå·²ä¿å­˜: {filepath}")

    def _save_markdown(self, papers: List[Dict], output_dir: Path, date_str: str):
        """ä¿å­˜ä¸ºMarkdownæ ¼å¼"""
        filepath = output_dir / f'{date_str}_results.md'
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"# arXivè®ºæ–‡è¿½è¸ªç»“æœ - {date_str}\n\n")
            f.write(f"**æŸ¥è¯¢æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
            f.write(f"**è®ºæ–‡æ•°é‡**: {len(papers)}\n\n")
            f.write("---\n\n")

            for paper in papers:
                f.write(f"## {paper['title']}\n\n")
                f.write(f"**arXiv ID**: `{paper['arxiv_id']}`  \n")
                f.write(f"**ä½œè€…**: {paper['authors']}  \n")
                f.write(f"**å‘å¸ƒæ—¥æœŸ**: {paper['published']}  \n")
                f.write(f"**åˆ†ç±»**: {paper['categories']}\n\n")
                f.write(f"**æ‘˜è¦**: {paper['summary']}\n\n")
                f.write(f"**é“¾æ¥**: [è®ºæ–‡é¡µé¢]({paper['url']}) | [PDFä¸‹è½½]({paper['pdf_url']})\n\n")
                f.write("---\n\n")
        print(f"ğŸ’¾ Markdownå·²ä¿å­˜: {filepath}")

    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        papers = self.search_papers()
        self.results = [self.parse_paper(p) for p in papers]
        self.output_results(self.results)


def load_config(config_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"âš ï¸  é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        return None


def interactive_config() -> Dict:
    """äº¤äº’å¼é…ç½®ç”Ÿæˆ"""
    print("\nğŸ”§ äº¤äº’å¼é…ç½®å‘å¯¼")
    print("=" * 50)

    config = {}

    # åˆ†ç±»é€‰æ‹©
    print("\nè¯·è¾“å…¥arXivåˆ†ç±»ï¼ˆç”¨ç©ºæ ¼åˆ†éš”ï¼Œç›´æ¥å›è½¦è·³è¿‡ï¼‰")
    print("å¸¸ç”¨åˆ†ç±»: cs.CV(è®¡ç®—æœºè§†è§‰) cs.LG(æœºå™¨å­¦ä¹ ) cs.AI(äººå·¥æ™ºèƒ½)")
    print("         cs.CL(è‡ªç„¶è¯­è¨€å¤„ç†) cs.CR(å¯†ç å­¦) stat.ML(ç»Ÿè®¡æœºå™¨å­¦ä¹ )")
    cats = input("åˆ†ç±»> ").strip()
    if cats:
        config['categories'] = cats.split()

    # å…³é”®è¯è¾“å…¥
    print("\nè¯·è¾“å…¥å…³é”®è¯ï¼ˆç”¨ç©ºæ ¼åˆ†éš”ï¼Œç›´æ¥å›è½¦è·³è¿‡ï¼‰")
    kws = input("å…³é”®è¯> ").strip()
    if kws:
        config['keywords'] = kws.split()
        print("\nå…³é”®è¯æœç´¢èŒƒå›´:")
        print("  1. æ ‡é¢˜ (ti:)")
        print("  2. æ‘˜è¦ (abs:)")
        print("  3. å…¨éƒ¨ (all:)")
        field_choice = input("é€‰æ‹© (é»˜è®¤3)> ").strip() or "3"
        field_map = {'1': ['ti:'], '2': ['abs:'], '3': ['all:']}
        config['keyword_fields'] = field_map.get(field_choice, ['all:'])

    # è®ºæ–‡æ•°é‡
    max_results = input("\nè·å–è®ºæ–‡æ•°é‡ (é»˜è®¤10)> ").strip() or "10"
    config['max_results'] = int(max_results)

    # è¾“å‡ºæ ¼å¼
    print("\nè¾“å‡ºæ ¼å¼ (å¤šé€‰ç”¨ç©ºæ ¼åˆ†éš”):")
    print("  console  - ç»ˆç«¯æ˜¾ç¤º")
    print("  txt      - æ–‡æœ¬æ–‡ä»¶")
    print("  md       - Markdownæ–‡ä»¶")
    print("  csv      - CSVæ–‡ä»¶")
    print("  json     - JSONæ–‡ä»¶")
    formats = input("æ ¼å¼ (é»˜è®¤: console)> ").strip() or "console"
    config['output_formats'] = formats.split()

    # é™é»˜æ¨¡å¼
    silent = input("\nå¯ç”¨é™é»˜æ¨¡å¼? (y/N, é»˜è®¤N)> ").strip().lower()
    config['silent'] = silent == 'y'

    return config


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(
        description='arXivæ™ºèƒ½è®ºæ–‡è¿½è¸ªå™¨ - åŸºäºèƒ¶æ°´ç¼–ç¨‹åŸåˆ™',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  %(prog)s --config config.json                    # ä½¿ç”¨é…ç½®æ–‡ä»¶
  %(prog)s --categories cs.CV cs.LG --keywords GPT # å‘½ä»¤è¡Œå‚æ•°
  %(prog)s --interactive                          # äº¤äº’å¼é…ç½®
  %(prog)s --config config.json --silent          # é™é»˜æ¨¡å¼

Crontabé…ç½®ç¤ºä¾‹:
  0 8 * * * /usr/bin/python3 %(prog)s --config config.json --silent
        """
    )

    parser.add_argument('--config', '-c', help='é…ç½®æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)')
    parser.add_argument('--categories', nargs='+', help='arXivåˆ†ç±» (å¦‚ cs.CV cs.LG)')
    parser.add_argument('--keywords', nargs='+', help='æœç´¢å…³é”®è¯')
    parser.add_argument('--keyword-fields', nargs='+', choices=['ti:', 'abs:', 'all:'],
                       default=['all:'], help='å…³é”®è¯æœç´¢å­—æ®µ')
    parser.add_argument('--max-results', type=int, default=10, help='æœ€å¤§ç»“æœæ•° (é»˜è®¤: 10)')
    parser.add_argument('--output-dir', '-o', default='outputs', help='è¾“å‡ºç›®å½• (é»˜è®¤: outputs)')
    parser.add_argument('--output-formats', nargs='+',
                       choices=['console', 'txt', 'md', 'csv', 'json'],
                       default=['console'], help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--history-file', default='papers_history.json', help='å†å²è®°å½•æ–‡ä»¶')
    parser.add_argument('--silent', '-s', action='store_true', help='é™é»˜æ¨¡å¼')
    parser.add_argument('--interactive', '-i', action='store_true', help='äº¤äº’å¼é…ç½®')
    parser.add_argument('--no-verify-ssl', action='store_true',
                       help='ç¦ç”¨SSLéªŒè¯ï¼ˆç”¨äºè¯ä¹¦é—®é¢˜ç¯å¢ƒï¼‰')

    args = parser.parse_args()

    # é…ç½®ä¼˜å…ˆçº§: é…ç½®æ–‡ä»¶ > å‘½ä»¤è¡Œå‚æ•° > äº¤äº’å¼
    config = {}

    # 1. å°è¯•åŠ è½½é…ç½®æ–‡ä»¶
    if args.config:
        config = load_config(args.config)
        if not config:
            if not args.silent:
                print("âŒ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")
            sys.exit(1)

    # 2. å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶
    if args.categories:
        config['categories'] = args.categories
    if args.keywords:
        config['keywords'] = args.keywords
        config['keyword_fields'] = args.keyword_fields
    if hasattr(args, 'max_results'):
        config['max_results'] = args.max_results
    if hasattr(args, 'output_dir'):
        config['output_dir'] = args.output_dir
    if hasattr(args, 'output_formats'):
        config['output_formats'] = args.output_formats
    if hasattr(args, 'history_file'):
        config['history_file'] = args.history_file
    if hasattr(args, 'silent'):
        config['silent'] = args.silent

    # 3. äº¤äº’å¼æ¨¡å¼
    if args.interactive or (not config and not args.categories and not args.keywords):
        if not args.silent:
            config = interactive_config()
            # ä¿å­˜äº¤äº’å¼é…ç½®
            config_path = 'config.json'
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            print(f"\nâœ… é…ç½®å·²ä¿å­˜åˆ° {config_path}")

    # è®¾ç½®é»˜è®¤å€¼
    config.setdefault('categories', [])
    config.setdefault('keywords', [])
    config.setdefault('keyword_fields', ['all:'])
    config.setdefault('max_results', 10)
    config.setdefault('output_dir', 'outputs')
    config.setdefault('output_formats', ['console'])
    config.setdefault('history_file', 'papers_history.json')
    config.setdefault('silent', False)
    config.setdefault('verify_ssl', not args.no_verify_ssl)

    # SSLé—®é¢˜æç¤º
    if not config['verify_ssl'] and not config['silent']:
        print("âš ï¸  SSLéªŒè¯å·²ç¦ç”¨ï¼Œä»…ç”¨äºè°ƒè¯•ç›®çš„")

    # åœ¨åˆ›å»ºtrackerä¹‹å‰è®¾ç½®SSLä¸Šä¸‹æ–‡
    setup_ssl_context(config.get('verify_ssl', True))

    # è¿è¡Œè¿½è¸ªå™¨
    try:
        tracker = ArxivTracker(config)
        if config['silent']:
            # é™é»˜æ¨¡å¼é‡å®šå‘è¾“å‡º
            import io
            import contextlib

            with contextlib.redirect_stdout(io.StringIO()):
                tracker.run()
            print(f"âœ… å®Œæˆï¼Œè·å– {len(tracker.results)} ç¯‡è®ºæ–‡")
        else:
            tracker.run()

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except requests.exceptions.SSLError as e:
        print("\nâŒ SSLè¿æ¥é”™è¯¯")
        print("è¿™å¯èƒ½æ˜¯ç”±ç½‘ç»œç¯å¢ƒæˆ–è¯ä¹¦é—®é¢˜å¯¼è‡´çš„")
        print("è¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š")
        print("  1. ä½¿ç”¨ --no-verify-ssl å‚æ•°ç¦ç”¨SSLéªŒè¯")
        print("  2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("  3. æ›´æ–°ç³»ç»Ÿè¯ä¹¦: sudo apt-get install ca-certificates")
        if not args.silent:
            print(f"\nè¯¦ç»†é”™è¯¯: {e}")
        sys.exit(1)
    except Exception as e:
        error_msg = str(e)
        print(f"\nâŒ é”™è¯¯: {error_msg}")

        # SSLé”™è¯¯æç¤º
        if 'SSL' in error_msg or 'EOF' in error_msg:
            print("ğŸ’¡ æç¤º: å°è¯•ä½¿ç”¨ --no-verify-ssl å‚æ•°")

        if not args.silent:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
